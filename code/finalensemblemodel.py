# -*- coding: utf-8 -*-
"""FinalEnsemblemodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14S9w7GPGoHkDXedfpqMx18sp_mBZRApM
"""

# Mount & installs
from google.colab import drive
drive.mount('/content/drive')
!pip -q install efficientnet tensorflow-addons
# Reproducibility
import os, random, numpy as np, tensorflow as tf
SEED = 42
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)
# Common imports
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from tensorflow.keras import layers, Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from efficientnet.tfkeras import EfficientNetB0

BASE = '/content/drive/MyDrive/Capstone2025_DeepfakeDetection'
DATA = f'{BASE}/data/frames_cropped_split'
MODEL_DIR = f'{BASE}/models_fast100'
os.makedirs(MODEL_DIR, exist_ok=True)

#Experiment hyperparameters
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 15

# Image preprocessing
# rescale pixel values from [0,255] to [0,1] for stable training
datagen = ImageDataGenerator(rescale=1./255)
train_generator = datagen.flow_from_directory(
os.path.join(DATA, 'train'), target_size=IMG_SIZE,
batch_size=BATCH_SIZE, class_mode='binary', shuffle=True, seed=SEED
)
val_generator = datagen.flow_from_directory(
os.path.join(DATA, 'val'), target_size=IMG_SIZE,
batch_size=BATCH_SIZE, class_mode='binary', shuffle=False
)
test_generator = datagen.flow_from_directory(
os.path.join(DATA, 'test'), target_size=IMG_SIZE,
batch_size=BATCH_SIZE, class_mode='binary', shuffle=False
)

# Optional: class weights
from collections import Counter
counts = Counter(train_generator.classes) # {0: real, 1: fake}
total = sum(counts.values())
class_weight = {0: total/(2.0*counts[0]), 1: total/(2.0*counts[1])}

#classification head on top of a frozen base
x = layers.GlobalAveragePooling2D()(base.output)
x = layers.BatchNormalization()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(dropout)(x)
out = layers.Dense(1, activation='sigmoid')(x)
return Model(base.input, out)

#build MobileNetV2 classifier
def build_mobilenetv2(input_shape=(224,224,3)):
base = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)
base.trainable = False # keep frozen for CPU speed
model = add_head(base, dropout=0.3)
model.compile(optimizer=Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])
return model

#build EfficientNetB0 classifier
def build_efficientnetb0(input_shape=(224,224,3)):
base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)
base.trainable = False # keep frozen for CPU speed
model = add_head(base, dropout=0.3)
model.compile(optimizer=Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])
return model

mnv2 = build_mobilenetv2()
enb0 = build_efficientnetb0()

#training callbacks
def get_callbacks(name):
return [
EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),
ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, min_lr=1e-6, verbose=1),
ModelCheckpoint(os.path.join(MODEL_DIR, f'{name}_best.keras'),
monitor='val_accuracy', save_best_only=True, verbose=1)
]

#plotting training curves
def plot_history(h, title):
acc, val_acc = h.history['accuracy'], h.history['val_accuracy']
loss, val_loss = h.history['loss'], h.history['val_loss']
rng = range(len(acc))
plt.figure(figsize=(12,4))
plt.subplot(1,2,1); plt.plot(rng, acc, label='train'); plt.plot(rng, val_acc, label='val'); plt.title(f'{title} Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()
plt.subplot(1,2,2); plt.plot(rng, loss, label='train'); plt.plot(rng, val_loss, label='val'); plt.title(f'{title} Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()
plt.show()

#train a single model end-to-end
def train_one(model, name, epochs=EPOCHS):
h = model.fit(
train_generator,
validation_data=val_generator,
epochs=epochs,
callbacks=get_callbacks(name),
class_weight=class_weight,
verbose=1
)
plot_history(h, name)
return model

print("Training MobileNetV2 (single-stage)...")
mnv2 = train_one(mnv2, 'mobilenetv2_fast', epochs=EPOCHS)
print("Training EfficientNetB0 (single-stage)...")
enb0 = train_one(enb0, 'efficientnetb0_fast', epochs=EPOCHS)

#Ensemble - weight sweep on val, evaluate on test
from tensorflow.keras.models import load_model
import numpy as np, json, time
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
def _load_best(name_fast, name_ft):
try:
return load_model(f"{MODEL_DIR}/{name_fast}.keras")
except Exception:
return load_model(f"{MODEL_DIR}/{name_ft}.keras")

# Load best checkpoints
mnv2_best = _load_best("mobilenetv2_fast_best", "mobilenetv2_ft_best")
enb0_best = _load_best("efficientnetb0_fast_best", "efficientnetb0_ft_best")

# val predictions (probabilities)
val_y = val_generator.classes
val_mnv2_p = mnv2_best.predict(val_generator, verbose=1)
val_enb0_p = enb0_best.predict(val_generator, verbose=1)

# sweep a on val (EfficientNetB0 weight = a, MobileNetV2 weight = 1-a)
alphas = np.linspace(0.0, 1.0, 21)
def pick_best_alpha(metric="f1"):
best_alpha, best_score = None, -1
for a in alphas:
probs = a*val_enb0_p + (1-a)*val_mnv2_p
preds = (probs >= 0.5).astype(int)
score = f1_score(val_y, preds, pos_label=1) if metric=="f1" else accuracy_score(val_y, preds)
if score > best_score:
best_alpha, best_score = float(a), float(score)
return best_alpha, best_score
alpha_f1, f1_best = pick_best_alpha(metric="f1")
alpha_acc, acc_best = pick_best_alpha(metric="acc")
print(f"\nBest α (by F1-Fake): {alpha_f1:.2f} | F1={f1_best:.4f}")
print(f"Best α (by Accuracy): {alpha_acc:.2f} | Acc={acc_best:.4f}")

ALPHA = alpha_acc
print(f"\n>> Using α={ALPHA:.2f} for TEST (EfficientNetB0 weight).")

# optional threshold tuning on val for the chosen a (keeps 0.5 if no gain)
def best_threshold(val_probs, y, metric_fn, sweep=np.linspace(0.3, 0.7, 17)):
best_t, best_m = 0.5, -1
for t in sweep:
preds = (val_probs >= t).astype(int)
m = metric_fn(y, preds)
if m > best_m:
best_t, best_m = float(t), float(m)
return best_t, best_m
val_probs_alpha = ALPHA*val_enb0_p + (1-ALPHA)*val_mnv2_p
t_opt, m_opt = best_threshold(val_probs_alpha, val_y, lambda y,p: accuracy_score(y,p))
print(f"Val-tuned threshold (for accuracy): {t_opt:.2f} (val-acc={m_opt:.4f})")

# test evaluation: (a) 50/50 baseline, (b) a-chosen + t_opt
test_y = test_generator.classes
test_mnv2_p = mnv2_best.predict(test_generator, verbose=1)
test_enb0_p = enb0_best.predict(test_generator, verbose=1)
# i) equal-weight baseline
test_probs_50 = 0.5*test_enb0_p + 0.5*test_mnv2_p
test_pred_50 = (test_probs_50 >= 0.5).astype(int)
acc_50 = accuracy_score(test_y, test_pred_50)
print("\n=== Equal-weight (0.50/0.50), threshold=0.50 ===")
print("Accuracy:", acc_50)
print(classification_report(test_y, test_pred_50, target_names=['Real','Fake']))
print("Confusion Matrix:\n", confusion_matrix(test_y, test_pred_50))
# ii) Weighted a + tuned threshold
test_probs_a = ALPHA*test_enb0_p + (1-ALPHA)*test_mnv2_p
test_pred_a = (test_probs_a >= t_opt).astype(int)
acc_a = accuracy_score(test_y, test_pred_a)
print(f"\n=== Weighted (α={ALPHA:.2f}), threshold={t_opt:.2f} ===")
print("Accuracy:", acc_a)
print(classification_report(test_y, test_pred_a, target_names=['Real','Fake']))
print("Confusion Matrix:\n", confusion_matrix(test_y, test_pred_a))

# save the chosen settings for the ensemble
summary = {
"alpha_used": ALPHA,
"threshold_used": t_opt,
"val_best_alpha_acc": {"alpha": alpha_acc, "val_acc": acc_best},
"val_best_alpha_f1": {"alpha": alpha_f1, "val_f1_fake": f1_best},
"test_equal_weight_acc": acc_50,
"test_weighted_acc": acc_a,
"timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
}
with open(f"{MODEL_DIR}/ensemble_summary.json", "w") as f:
json.dump(summary, f, indent=2)
print(f"\nSaved summary → {MODEL_DIR}/ensemble_summary.json")

#rebuild and save ensemble
# paths
BASE = '/content/drive/MyDrive/Capstone2025_DeepfakeDetection'
MODEL_DIR = f'{BASE}/models_fast100'
# imports + custom objects for EfficientNet checkpoints
import os, json, numpy as np, tensorflow as tf
from tensorflow.keras.models import load_model
tf.keras.utils.get_custom_objects().update({
"swish": tf.keras.activations.swish,
"FixedDropout": tf.keras.layers.Dropout,
})

# load the two best checkpoints
mnv2 = load_model(os.path.join(MODEL_DIR, 'mobilenetv2_fast_best.keras'), compile=False)
enb0 = load_model(os.path.join(MODEL_DIR, 'efficientnetb0_fast_best.keras'), compile=False)
mnv2.trainable = False
enb0.trainable = False
# sanity check - same input shape
assert mnv2.input_shape[1:] == enb0.input_shape[1:], \
f"Input shapes differ: {mnv2.input_shape} vs {enb0.input_shape}"

# helper
def to_prob_1d(tensor, prefix):
ch = int(tensor.shape[-1])
if ch == 1:
return tensor
elif ch == 2:
s = tf.keras.layers.Softmax(axis=-1, name=f'{prefix}_softmax')(tensor)
take1 = tf.keras.layers.Dense(1, use_bias=False, trainable=False,
name=f'{prefix}_take_class1')

# build and set fixed selector weights [[0],[1]]
take1.build((None, 2))
take1.set_weights([np.array([[0.0],[1.0]], dtype='float32')])
return take1(s)
else:
raise ValueError(f"{prefix}: unexpected head size {ch}; expected 1 or 2.")

# read ALPHA chosen on validation (fallback to 0.5)
ALPHA = 0.5
sum_path = os.path.join(MODEL_DIR, 'ensemble_summary.json')
if os.path.exists(sum_path):
try:
with open(sum_path, 'r') as f:
ALPHA = float(json.load(f).get('alpha_used', 0.5))
except Exception:
ALPHA = 0.5

# build the functional ensemble graph
inp = tf.keras.Input(shape=mnv2.input_shape[1:], name="image")
p1 = mnv2(inp, training=False)
p2 = enb0(inp, training=False)
p1 = to_prob_1d(p1, 'mnv2')
p2 = to_prob_1d(p2, 'enb0')

# weighted average via Rescaling
s1 = tf.keras.layers.Rescaling(1.0 - ALPHA, name="scale_mnv2")(p1)
s2 = tf.keras.layers.Rescaling(ALPHA, name="scale_enb0")(p2)
out = tf.keras.layers.Add(name="weighted_avg_probs")([s1, s2])
ensemble = tf.keras.Model(inputs=inp, outputs=out, name="ensemble_mnv2_enb0_safe")

# save ensemble
ENS_PATH = os.path.join(MODEL_DIR, "ensemble_mnv2_enb0_best.keras")
ensemble.save(ENS_PATH)
print(f"Saved single-file ensemble to: {ENS_PATH}")

# quick load sanity
ens_loaded = load_model(ENS_PATH, compile=False)
print("Reloaded ensemble OK:", ens_loaded.name)

# load best checkpoints
from tensorflow.keras.models import load_model
import numpy as np, os, cv2, matplotlib.pyplot as plt
import tensorflow as tf
mnv2 = load_model(os.path.join(MODEL_DIR, 'mobilenetv2_fast_best.keras'))
enb0 = load_model(os.path.join(MODEL_DIR, 'efficientnetb0_fast_best.keras'))

# Label names
idx_to_label = {v:k for k,v in test_generator.class_indices.items()}
# save figures
GC_DIR = os.path.join(MODEL_DIR, 'gradcam')
os.makedirs(GC_DIR, exist_ok=True)
def get_last_conv_layer(model):
# Find the last layer
for layer in reversed(model.layers):
  if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.DepthwiseConv2D)):
return layer.name
# Fallback - try to guess common names
candidates = ['Conv_1', 'block7a_project_conv']
for name in candidates:
if name in [l.name for l in model.layers]:
return name
raise ValueError("No convolutional layer found for Grad-CAM.")

def compute_gradcam(model, img_array, layer_name=None, eps=1e-8):
if layer_name is None:
layer_name = get_last_conv_layer(model)
grad_model = tf.keras.models.Model(
[model.inputs],
[model.get_layer(layer_name).output, model.output]
)
with tf.GradientTape() as tape:
inputs = tf.cast(img_array[None, ...], tf.float32) # (1,H,W,3)
conv_outputs, preds = grad_model(inputs)
# Binary classifier
loss = preds[:, 0]
grads = tape.gradient(loss, conv_outputs) # (1,h,w,c)
# Global-average-pool the gradients over spatial dims
pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)) # (c,)
conv_outputs = conv_outputs[0] # (h,w,c)
heatmap = tf.reduce_sum(tf.multiply(conv_outputs, pooled_grads), axis=-1) # (h,w)
heatmap = tf.nn.relu(heatmap)
heatmap = heatmap / (tf.reduce_max(heatmap) + eps)
heatmap = heatmap.numpy()
# Resize to input size
h, w = img_array.shape[:2]
heatmap = cv2.resize(heatmap, (w, h), interpolation=cv2.INTER_CUBIC)
return heatmap

def overlay_heatmap(img, heatmap, alpha=0.35):
cmap = cv2.applyColorMap((heatmap*255).astype(np.uint8), cv2.COLORMAP_JET)
cmap = cv2.cvtColor(cmap, cv2.COLOR_BGR2RGB) / 255.0
overlay = (1 - alpha) * img + alpha * cmap
overlay = np.clip(overlay, 0, 1)
return overlay

def predict_prob(model, img):
# img is [0,1], add batch
p = model.predict(img[None, ...], verbose=0)[0,0]
return float(p)

def show_and_save_gradcam(img, label, fname_prefix="sample"):
# img, label from generator batch (img already rescaled to [0,1])
# Individual model CAMs
cam_mnv2 = compute_gradcam(mnv2, img)
cam_enb0 = compute_gradcam(enb0, img)
# average of normalized heatmaps
cam_ens = (cam_mnv2 + cam_enb0) / 2.0
# Overlays
ov_mnv2 = overlay_heatmap(img, cam_mnv2)
ov_enb0 = overlay_heatmap(img, cam_enb0)
ov_ens = overlay_heatmap(img, cam_ens)
# Predictions
p_mnv2 = predict_prob(mnv2, img)
p_enb0 = predict_prob(enb0, img)
p_ens = (p_mnv2 + p_enb0) / 2.0
# Titles
gt = idx_to_label[int(label)]
pred_m = f"MobileNetV2: p(fake)={p_mnv2:.3f}"
pred_e = f"EfficientNetB0: p(fake)={p_enb0:.3f}"
pred_s = f"Ensemble: p(fake)={p_ens:.3f}"