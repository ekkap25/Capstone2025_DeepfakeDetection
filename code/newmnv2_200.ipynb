{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoESqBn_kIy_",
        "outputId": "40162935-8ce0-46ec-c157-3c752dfd48eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Repro + imports\n",
        "import os, random, numpy as np, tensorflow as tf\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "# Paths (point to dataset_200)\n",
        "BASE = '/content/drive/MyDrive/Capstone2025_DeepfakeDetection'\n",
        "DATA = f'{BASE}/data/dataset_200/frames_cropped_split'   # <-- your new dataset\n",
        "MODEL_DIR = f'{BASE}/models_fast200'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "IMG_SIZE   = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS     = 15\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple rescale-only (same as before)\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    os.path.join(DATA, 'train'),\n",
        "    target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='binary', shuffle=True, seed=SEED\n",
        ")\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    os.path.join(DATA, 'val'),\n",
        "    target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='binary', shuffle=False\n",
        ")\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    os.path.join(DATA, 'test'),\n",
        "    target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='binary', shuffle=False\n",
        ")\n",
        "\n",
        "# Optional: class weights (keeps recall healthier if slightly imbalanced)\n",
        "from collections import Counter\n",
        "counts = Counter(train_generator.classes)  # {0: real, 1: fake}\n",
        "total = sum(counts.values())\n",
        "class_weight = {0: total/(2.0*counts[0]), 1: total/(2.0*counts[1])}\n",
        "class_weight\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR1frElUk7fe",
        "outputId": "71b57825-b1e6-4704-b1bf-fb2f77baef61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5600 images belonging to 2 classes.\n",
            "Found 1200 images belonging to 2 classes.\n",
            "Found 1200 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.0, 1: 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_head(base, dropout=0.3):\n",
        "    x = layers.GlobalAveragePooling2D()(base.output)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "    return Model(base.input, out)\n",
        "\n",
        "def build_mobilenetv2(input_shape=(224,224,3)):\n",
        "    base = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base.trainable = False   # freeze for a fast first stage (CPU-friendly)\n",
        "    model = add_head(base, dropout=0.3)\n",
        "    model.compile(optimizer=Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "bxldYXqGlzYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_callbacks(name):\n",
        "    return [\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, min_lr=1e-6, verbose=1),\n",
        "        ModelCheckpoint(os.path.join(MODEL_DIR, f'{name}_best.keras'),\n",
        "                        monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "rOJwnOV5l0vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnv2 = build_mobilenetv2()\n",
        "\n",
        "history = mnv2.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=get_callbacks('mobilenetv2_fast'),\n",
        "    class_weight=class_weight,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP1EzhFfl43T",
        "outputId": "a3268134-a600-4b98-bd63-e2be05d9f476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - accuracy: 0.7124 - loss: 0.6403 \n",
            "Epoch 1: val_accuracy improved from -inf to 0.83500, saving model to /content/drive/MyDrive/Capstone2025_DeepfakeDetection/models_fast200/mobilenetv2_fast_best.keras\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2678s\u001b[0m 15s/step - accuracy: 0.7127 - loss: 0.6397 - val_accuracy: 0.8350 - val_loss: 0.3656 - learning_rate: 0.0010\n",
            "Epoch 2/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816ms/step - accuracy: 0.8624 - loss: 0.2969\n",
            "Epoch 2: val_accuracy improved from 0.83500 to 0.88583, saving model to /content/drive/MyDrive/Capstone2025_DeepfakeDetection/models_fast200/mobilenetv2_fast_best.keras\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1s/step - accuracy: 0.8625 - loss: 0.2968 - val_accuracy: 0.8858 - val_loss: 0.2551 - learning_rate: 0.0010\n",
            "Epoch 3/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809ms/step - accuracy: 0.9026 - loss: 0.2183\n",
            "Epoch 3: val_accuracy improved from 0.88583 to 0.89750, saving model to /content/drive/MyDrive/Capstone2025_DeepfakeDetection/models_fast200/mobilenetv2_fast_best.keras\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 985ms/step - accuracy: 0.9026 - loss: 0.2184 - val_accuracy: 0.8975 - val_loss: 0.2354 - learning_rate: 0.0010\n",
            "Epoch 4/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946ms/step - accuracy: 0.9181 - loss: 0.1881\n",
            "Epoch 4: val_accuracy improved from 0.89750 to 0.90250, saving model to /content/drive/MyDrive/Capstone2025_DeepfakeDetection/models_fast200/mobilenetv2_fast_best.keras\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 1s/step - accuracy: 0.9181 - loss: 0.1881 - val_accuracy: 0.9025 - val_loss: 0.2092 - learning_rate: 0.0010\n",
            "Epoch 5/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901ms/step - accuracy: 0.9230 - loss: 0.1650\n",
            "Epoch 5: val_accuracy improved from 0.90250 to 0.90750, saving model to /content/drive/MyDrive/Capstone2025_DeepfakeDetection/models_fast200/mobilenetv2_fast_best.keras\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 1s/step - accuracy: 0.9230 - loss: 0.1650 - val_accuracy: 0.9075 - val_loss: 0.2101 - learning_rate: 0.0010\n",
            "Epoch 6/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888ms/step - accuracy: 0.9362 - loss: 0.1406\n",
            "Epoch 6: val_accuracy improved from 0.90750 to 0.91250, saving model to /content/drive/MyDrive/Capstone2025_DeepfakeDetection/models_fast200/mobilenetv2_fast_best.keras\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 1s/step - accuracy: 0.9362 - loss: 0.1407 - val_accuracy: 0.9125 - val_loss: 0.1989 - learning_rate: 0.0010\n",
            "Epoch 7/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890ms/step - accuracy: 0.9344 - loss: 0.1417\n",
            "Epoch 7: val_accuracy did not improve from 0.91250\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 1s/step - accuracy: 0.9344 - loss: 0.1417 - val_accuracy: 0.9075 - val_loss: 0.2044 - learning_rate: 0.0010\n",
            "Epoch 8/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884ms/step - accuracy: 0.9370 - loss: 0.1276\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 8: val_accuracy did not improve from 0.91250\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 1s/step - accuracy: 0.9370 - loss: 0.1276 - val_accuracy: 0.9083 - val_loss: 0.2094 - learning_rate: 0.0010\n",
            "Epoch 9/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875ms/step - accuracy: 0.9460 - loss: 0.1130\n",
            "Epoch 9: val_accuracy did not improve from 0.91250\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 1s/step - accuracy: 0.9460 - loss: 0.1130 - val_accuracy: 0.9117 - val_loss: 0.1823 - learning_rate: 3.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884ms/step - accuracy: 0.9442 - loss: 0.1057\n",
            "Epoch 10: val_accuracy improved from 0.91250 to 0.91500, saving model to /content/drive/MyDrive/Capstone2025_DeepfakeDetection/models_fast200/mobilenetv2_fast_best.keras\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 1s/step - accuracy: 0.9443 - loss: 0.1056 - val_accuracy: 0.9150 - val_loss: 0.1743 - learning_rate: 3.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879ms/step - accuracy: 0.9493 - loss: 0.1012\n",
            "Epoch 11: val_accuracy did not improve from 0.91500\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 1s/step - accuracy: 0.9493 - loss: 0.1011 - val_accuracy: 0.9125 - val_loss: 0.1808 - learning_rate: 3.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879ms/step - accuracy: 0.9482 - loss: 0.1000\n",
            "Epoch 12: val_accuracy did not improve from 0.91500\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 1s/step - accuracy: 0.9482 - loss: 0.1000 - val_accuracy: 0.9133 - val_loss: 0.1722 - learning_rate: 3.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886ms/step - accuracy: 0.9559 - loss: 0.0823\n",
            "Epoch 13: val_accuracy improved from 0.91500 to 0.91917, saving model to /content/drive/MyDrive/Capstone2025_DeepfakeDetection/models_fast200/mobilenetv2_fast_best.keras\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 1s/step - accuracy: 0.9558 - loss: 0.0824 - val_accuracy: 0.9192 - val_loss: 0.1743 - learning_rate: 3.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873ms/step - accuracy: 0.9575 - loss: 0.0839\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 14: val_accuracy did not improve from 0.91917\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 1s/step - accuracy: 0.9576 - loss: 0.0839 - val_accuracy: 0.9183 - val_loss: 0.1802 - learning_rate: 3.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876ms/step - accuracy: 0.9616 - loss: 0.0807\n",
            "Epoch 15: val_accuracy did not improve from 0.91917\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 1s/step - accuracy: 0.9616 - loss: 0.0807 - val_accuracy: 0.9117 - val_loss: 0.1768 - learning_rate: 9.0000e-05\n",
            "Epoch 15: early stopping\n",
            "Restoring model weights from the end of the best epoch: 12.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np, json, time\n",
        "\n",
        "# Load the best checkpoint if saved\n",
        "import tensorflow as tf, os\n",
        "best_path = os.path.join(MODEL_DIR, 'mobilenetv2_ft_best.keras')\n",
        "if not os.path.exists(best_path):\n",
        "    best_path = os.path.join(MODEL_DIR, 'mobilenetv2_fast_best.keras')\n",
        "mnv2_best = tf.keras.models.load_model(best_path)\n",
        "\n",
        "# Evaluate\n",
        "test_probs = mnv2_best.predict(test_generator, verbose=1)\n",
        "test_pred  = (test_probs >= 0.5).astype(int).ravel()  # keep 0.5 default; easy to change later\n",
        "y_true     = test_generator.classes\n",
        "\n",
        "print(\"Accuracy:\", (test_pred == y_true).mean())\n",
        "print(classification_report(y_true, test_pred, target_names=['Real','Fake']))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, test_pred))\n",
        "\n",
        "# Save a single-file model for deployment\n",
        "single_path = os.path.join(MODEL_DIR, 'mobilenetv2_final.keras')\n",
        "mnv2_best.save(single_path)\n",
        "print(\"Saved model →\", single_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4JKeNHkClZN",
        "outputId": "dee0a8b1-dd89-4652-f514-02cab1b78486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m773s\u001b[0m 21s/step\n",
            "Accuracy: 0.9041666666666667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.89      0.92      0.91       600\n",
            "        Fake       0.92      0.89      0.90       600\n",
            "\n",
            "    accuracy                           0.90      1200\n",
            "   macro avg       0.90      0.90      0.90      1200\n",
            "weighted avg       0.90      0.90      0.90      1200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[551  49]\n",
            " [ 66 534]]\n",
            "Saved model → /content/drive/MyDrive/Capstone2025_DeepfakeDetection/models_fast200/mobilenetv2_final.keras\n"
          ]
        }
      ]
    }
  ]
}